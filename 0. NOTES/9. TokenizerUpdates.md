To update your `Tokenizer` to leverage the new `CharacterStream` functions, you should shift the responsibility of "peeking" and "marking" into the lexing loop. This allows you to handle complex tokens—like multi-character operators or specific CSS units—without prematurely advancing the stream.

Here is how to refactor your `Tokenizer` to use the advanced features of your `CharacterStream`.

### 1. Simplify the Main Loop

Instead of a standard `for...of` loop, use a `while` loop with `stream.next()`. This gives you the flexibility to use `mark()`, `reset()`, and `peek()` inside the loop body.

```typescript
// src/Tokenizer.ts
public tokenize(input: string): Token[] {
    const stream = new Char.Stream(input);
    const tokens: Token[] = [];
    this.buffer = [];

    while (!stream.isEOF()) {
        const wasInString = this.ctx.isInString();
        
        // Use the new next() method to get the full Character object
        const charResult = stream.next();
        if (charResult.done) break;
        const char = charResult.value;

        let result = this.ctx.process(char);

        // Handle emission before potentially reprocessing
        if (result.emit && this.buffer.length > 0) {
            tokens.push(this.createTokenFromBuffer(wasInString));
        }

        if (result.reprocess) {
            // We can now use stream.back() or just let the context 
            // handle the re-processing of the current char
            result = this.ctx.process(char);
        }

        if (result.action === 'buffer' && char.type !== Char.Type.EOF) {
            this.buffer.push(char);
        }
    }

    // Flush final token
    if (this.buffer.length > 0) {
        tokens.push(this.createTokenFromBuffer(this.ctx.isInString()));
    }

    tokens.push({ value: '', type: TokenType.EOF });
    return tokens;
}

```

### 2. Using `peek()` for Multi-character Symbols

If you want to support operators like `==` or `!=` (common in many languages), you can use the `peek()` function to look ahead without moving the cursor.

```typescript
// Inside a State handler or the Tokenizer loop
if (char.value === '=') {
    const nextChar = stream.peek(0); // Look at the very next char
    if (nextChar.value === '=') {
        stream.next(); // Actually consume the second '='
        // Logic for TokenType.EQUALS_EQUALS
    }
}

```

### 3. Using `mark()` and `reset()` for Speculative Units

If your language has ambiguous syntax where a sequence might be a `Dimension` or just a `Number` followed by an `Identifier`, you can use speculative marks.

```typescript
private handleSpeculativeUnit(stream: Char.Stream) {
    stream.mark(); // Save position
    
    // Try to consume something that looks like a complex unit
    const unitChars = stream.consumeWhile(c => c.type === Char.Type.Letter);
    
    if (this.isValidUnit(unitChars)) {
        stream.commit(); // Success! Keep the progress
    } else {
        stream.reset();  // Failure! Rewind to before we started consuming letters
    }
}

```

### 4. Leveraging `lookbackWhile` for Context

Your `Tokenizer` can now make decisions based on what happened *before* the current character without tracking it manually in a separate array. For example, to determine if a `-` is a "Unary Minus" or a "Subtraction" operator:

```typescript
const isUnary = (stream: Char.Stream) => {
    // Look back past any whitespace to see if the previous token was an operator
    const preceding = stream.lookbackWhile(c => c.type === Char.Type.Whitespace);
    const lastNonSpace = stream.charsBuffer[stream.charsBuffer.length - preceding.length - 2];
    
    return lastNonSpace?.type === Char.Type.LParen || lastNonSpace?.type === Char.Type.Plus;
};

```

### Key Efficiency Gains:

* **Coordinate Accuracy**: By using `stream.next()`, your tokens will now have perfect `line` and `column` data derived directly from the stream's position.
* **Cleanliness**: You no longer need to manually manage `index` or `line` increments inside the `Tokenizer`; the `Stream` class handles the internal state of the source string.
* **Safety**: Using `normalize('NFC')` in the stream constructor ensures that characters like `e` + `´` are treated as a single `é` character, preventing bugs in your classification logic.
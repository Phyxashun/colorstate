# lookbackWhile() Method

Yes, absolutely. That's a fantastic idea and a very common requirement for more advanced tokenizers. A lookbackWhile method is the perfect complement to consumeWhile.

***

This pattern allows the tokenizer to react to a "terminating" character (like %, px, ", etc.) and then retrospectively bundle the preceding characters into a single, more meaningful token.

Your charsBuffer is the key to making this work. It acts as the history that lookbackWhile will inspect.

## The lookbackWhile Method Implementation

Here is the method. You can add it directly to your CharacterStream class. It will iterate backwards from the end of your charsBuffer.

```typescript
// Add this method to your CharacterStream class

/**
 * Looks backwards from the current stream position through the character buffer
 * and collects a contiguous sequence of characters that match a predicate.
 *
 * This method does NOT change the stream's position.
 *
 * @param predicate A function that takes a Character and returns a boolean.
 * @returns An array of the matching characters, in their original forward order.
 */
public lookbackWhile(predicate: (char: Character) => boolean): Character[] {
    if (this.charsBuffer.length === 0) {
        return [];
    }

    const lookedBackChars: Character[] = [];

    // Iterate backwards from the end of the buffer.
    for (let i = this.charsBuffer.length - 1; i >= 0; i--) {
        const char = this.charsBuffer[i];

        // If the character matches the predicate, add it to our results.
        if (predicate(char)) {
            lookedBackChars.push(char);
        } else {
            // If we find a character that doesn't match, we must stop.
            // We only want a contiguous block from the end.
            break;
        }
    }

    // The characters were collected in reverse order (e.g., '3', '2', '1').
    // Reverse the array to return them in their natural order ('1', '2', '3').
    return lookedBackChars.reverse();
}
```

## How to Use It in a Tokenizer

Now, let's create a conceptual tokenizer that uses this new method to handle your exact use case: combining numbers into a single "percent" token when it encounters a %.

The key insight here is that by the time you see the %, you have already processed the numbers and likely added them to your token list as individual Number tokens. Your tokenizer needs to correct its own output by removing those individual tokens and replacing them with the new, combined one.

## Here is a complete, runnable example:

```ts
// Assume your CharacterStream, CharType, etc., are all defined.

function createPercentToken(stream: CharacterStream) {
    // 1. We are at the '%' character. Look backwards for numbers.
    const numberPart = stream.lookbackWhile(char => char.type === CharType.Number);

    // If we didn't find any numbers, just return the percent sign itself.
    if (numberPart.length === 0) {
        return { type: 'PERCENT_SIGN', value: '%' };
    }

    // 2. We found numbers! Combine them to form the new token.
    const numberValue = numberPart.map(c => c.value).join('');
    const fullTokenValue = numberValue + '%';

    // 3. Create the new, combined token.
    // The `replaceCount` tells the tokenizer how many previous tokens to discard.
    return {
        type: 'PERCENT_TOKEN',
        value: fullTokenValue,
        replaceCount: numberPart.length, // VERY IMPORTANT
    };
}
```

```ts
// --- A Simple Tokenizer ---
function tokenizer(stream: CharacterStream) {
    const tokens: any[] = [];

    while (!stream.isEOF()) {
        const char = stream.peek();

        // --- The Special Logic for '%' ---
        if (char.value === '%') {
            // Create the special token.
            const token = createPercentToken(stream);

            // If the token wants us to replace previous tokens, do it now.
            if (token.replaceCount && token.replaceCount > 0) {
                // Remove the now-redundant individual number tokens from our output.
                tokens.splice(tokens.length - token.replaceCount);
                console.log(`⏪ Corrected history: Removed ${token.replaceCount} previous number token(s).`);
            }
            
            // Add the new, combined token.
            tokens.push({ type: token.type, value: token.value });

            stream.next(); // Consume the '%'
            continue;
        }

        // --- Default Logic for All Other Characters ---
        // For any other character, just add it as a simple token.
        tokens.push({ type: char.type, value: char.value });
        stream.next();
    }
    return tokens;
}
```

```ts
// --- Run the Example ---
const input = "Total progress: 100%";
const stream = new CharacterStream(input);

console.log(`Input: "${input}"\n`);
const finalTokens = tokenizer(stream);

console.log("\n✅ Final Tokens:");
console.table(finalTokens);
```

### Output of the Example:

```bash
Input: "Total progress: 100%"

Corrected history: Removed 3 previous number token(s).

Final Tokens:
┌─────────┬────────────────┬───────────────┐
│ (index) │      type      │     value     │
├─────────┼────────────────┼───────────────┤
│    0    │    'Letter'    │      'T'      │
│    1    │    'Letter'    │      'o'      │
│    2    │    'Letter'    │      't'      │
│    3    │    'Letter'    │      'a'      │
│    4    │    'Letter'    │      'l'      │
│    5    │  'Whitespace'  │      ' '      │
│    6    │    'Letter'    │      'p'      │
│   ...   │       ...      │      ...      │
│   15    │  'Punctuation' │      ':'      │
│   16    │  'Whitespace'  │      ' '      │
│   17    │ 'PERCENT_TOKEN'│     '100%'    │
└─────────┴────────────────┴───────────────┘
```

This "look back and correct" pattern is extremely powerful. Your lookbackWhile method provides the essential tool to inspect the immediate past, enabling your tokenizer to build context-aware, complex tokens from simple ones.

***

### Suggestions

* How would I implement `mark()` and `reset()` with this new `charsBuffer`?
* Can you show me an example of `back(steps)` in action with the `charsBuffer`?
* How can I make `lookbackWhile` more efficient for very long inputs?
